name: CLREPLAYCNN
description: Replay-based continual learning for CNN models with ResNet support.

inputs:
  - name: processed_data_pickle
    type: Dataset

  - name: previous_model
    type: Model
    description: "Optional previous model checkpoint"
    optional: true

  - name: previous_replay_buffer
    type: String
    description: "Optional previous replay buffer pickle"
    optional: true

  - name: config_str
    type: String
    description: "Training + replay configuration JSON string"

outputs:
  - name: updated_model
    type: Model
  - name: updated_replay_buffer
    type: Dataset
  - name: training_report
    type: String

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        # Install common ML packages (optional; remove or pin as needed)
        python3 -m pip install --quiet --upgrade pip setuptools wheel
        python3 -m pip install --quiet "numpy<2,>=1.22" "pillow" "torch" "torchvision==0.17.0"
        # Hand over control to the next command in the list
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os, json, torch, pickle, argparse, random
        import torch.nn as nn
        import torch.optim as optim
        from torchvision import models

        parser = argparse.ArgumentParser()
        parser.add_argument("--processed_data_pickle")
        parser.add_argument("--previous_model", required=False)
        parser.add_argument("--previous_replay_buffer", required=False)
        parser.add_argument("--config_str")
        parser.add_argument("--updated_model")
        parser.add_argument("--updated_replay_buffer")
        parser.add_argument("--training_report")
        args = parser.parse_args()

        # -------------------------------
        # === Dataset / wrapper classes for unpickling ===
        # -------------------------------
        class LabeledDataset:
            def __init__(self, *args, **kwargs):
                obj = None
                if "dataset" in kwargs:
                    obj = kwargs["dataset"]
                elif len(args) == 1:
                    obj = args[0]
                else:
                    obj = kwargs

                self.dataset = getattr(obj, "dataset", None) or \
                               getattr(obj, "data", None) or \
                               getattr(obj, "samples", None) or \
                               obj

            def __len__(self):
                try:
                    return len(self.dataset)
                except:
                    return 100

            def __getitem__(self, idx):
                try:
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    if isinstance(item, dict):
                        return item.get("image_data"), item.get("label", 0)
                except:
                    pass
                return torch.randn(3,224,224), 0

        class CustomJSONDataset(LabeledDataset):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)

        class DataWrapper:
            def __init__(self, d=None):
                if d:
                    self.__dict__.update(d)

        # -------------------------------
        # === Model Architecture Detection ===
        # -------------------------------
        def detect_architecture(state_dict):
            '''
            Detect if state_dict is from ResNet or SimpleCNN
            '''
            keys = list(state_dict.keys())
            
            # Check for ResNet patterns
            if any('layer1' in k or 'layer2' in k for k in keys):
                return 'resnet'
            # Check for SimpleCNN patterns
            elif any('features' in k or 'classifier' in k for k in keys):
                return 'simplecnn'
            else:
                return 'unknown'

        # -------------------------------
        # === SimpleCNN Definition ===
        # -------------------------------
        class SimpleCNN(nn.Module):
            def __init__(self, num_classes):
                super().__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
                    nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)
                )
                self.classifier = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(64*56*56, 128),
                    nn.ReLU(),
                    nn.Linear(128, num_classes)
                )

            def forward(self, x):
                x = self.features(x)
                return self.classifier(x)

        # -------------------------------
        # === ResNet-based Model ===
        # -------------------------------
        class ResNetClassifier(nn.Module):
            def __init__(self, num_classes, resnet_type='resnet18'):
                super().__init__()
                # Create base ResNet
                if resnet_type == 'resnet18':
                    self.backbone = models.resnet18(weights=None)
                elif resnet_type == 'resnet34':
                    self.backbone = models.resnet34(weights=None)
                elif resnet_type == 'resnet50':
                    self.backbone = models.resnet50(weights=None)
                else:
                    self.backbone = models.resnet18(weights=None)
                
                # Replace final FC layer
                in_features = self.backbone.fc.in_features
                self.backbone.fc = nn.Linear(in_features, num_classes)

            def forward(self, x):
                return self.backbone(x)

        # -------------------------------------------------------
        # Load config
        # -------------------------------------------------------
        cfg = json.loads(args.config_str)

        replay_size = cfg["training"].get("replay_buffer_size", 500)
        batch_size = cfg["training"].get("batch_size", 32)
        lr = cfg["training"].get("optimizer", {}).get("learning_rate", 0.001)
        epochs = cfg["training"].get("epochs", 3)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Model architecture preference (can be set in config)
        model_type = cfg.get("model_architecture", "auto")  # auto, resnet18, resnet34, resnet50, simplecnn

        print(f"Using device: {device}")

        # -------------------------------------------------------
        # Load new processed data
        # -------------------------------------------------------
        with open(args.processed_data_pickle, "rb") as f:
            processed = pickle.load(f)

        train_loader = processed.train_loader
        num_classes = processed.num_classes
        print(f"Loaded data: {num_classes} classes")

        # -------------------------------------------------------
        # Determine architecture from previous model or config
        # -------------------------------------------------------
        architecture = None
        previous_state_dict = None

        if args.previous_model and os.path.exists(args.previous_model):
            print("Loading previous model checkpoint...")
            previous_state_dict = torch.load(args.previous_model, map_location="cpu")
            detected = detect_architecture(previous_state_dict)
            print(f"Detected architecture: {detected}")
            
            if model_type == "auto":
                architecture = detected
            else:
                architecture = model_type
                print(f"Overriding with config architecture: {architecture}")
        else:
            print("No previous model provided")
            if model_type == "auto":
                architecture = "resnet18"  # Default
            else:
                architecture = model_type
            print(f"Using architecture: {architecture}")

        # -------------------------------------------------------
        # Build Model based on detected/configured architecture
        # -------------------------------------------------------
        if architecture == 'simplecnn':
            print("Creating SimpleCNN model")
            model = SimpleCNN(num_classes).to(device)
        elif architecture in ['resnet', 'resnet18', 'resnet34', 'resnet50']:
            resnet_type = architecture if architecture.startswith('resnet') else 'resnet18'
            print(f"Creating {resnet_type} model")
            model = ResNetClassifier(num_classes, resnet_type).to(device)
        else:
            print(f"Unknown architecture '{architecture}', defaulting to ResNet18")
            model = ResNetClassifier(num_classes, 'resnet18').to(device)

        # -------------------------------------------------------
        # Load previous model weights
        # -------------------------------------------------------
        if previous_state_dict is not None:
            try:
                model.load_state_dict(previous_state_dict, strict=False)
                print("✓ Successfully loaded previous model weights (strict=False)")
            except Exception as e:
                print(f"⚠ Warning: Could not load previous weights: {e}")
                print("Continuing with randomly initialized model")

        # -------------------------------------------------------
        # Load previous replay buffer (optional)
        # -------------------------------------------------------
        replay_buffer = []

        if args.previous_replay_buffer and os.path.exists(args.previous_replay_buffer):
            print("Loading previous replay buffer...")
            try:
                with open(args.previous_replay_buffer, "rb") as f:
                    replay_buffer = pickle.load(f)
                print(f"Loaded {len(replay_buffer)} samples from replay buffer")
            except Exception as e:
                print(f"⚠ Warning: Could not load replay buffer: {e}")
        else:
            print("No previous replay buffer provided")

        optimizer = optim.Adam(model.parameters(), lr=lr)
        loss_fn = nn.CrossEntropyLoss()

        # -------------------------------------------------------
        # Convert replay buffer → DataLoader
        # -------------------------------------------------------
        def build_replay_loader(buffer):
            if len(buffer) == 0:
                return None

            tensor_x = torch.stack([item[0] for item in buffer])
            tensor_y = torch.tensor([item[1] for item in buffer])

            ds = torch.utils.data.TensorDataset(tensor_x, tensor_y)
            return torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)

        replay_loader = build_replay_loader(replay_buffer)

        # -------------------------------------------------------
        # Training loop with replay
        # -------------------------------------------------------
        report = {
            "epochs": epochs,
            "architecture": architecture,
            "num_classes": num_classes,
            "replay_buffer_size": len(replay_buffer),
            "loss_per_epoch": [],
            "accuracy_per_epoch": []
        }

        print(f"Starting training for {epochs} epochs...")
        print(f"Replay buffer: {len(replay_buffer)} samples")
        print(f"New data batches: {len(train_loader)}")
        print("-" * 50)

        for ep in range(epochs):
            model.train()
            total_loss = 0
            correct = 0
            total = 0
            batch_count = 0

            # Iterate through new data
            for x, y in train_loader:
                x, y = x.to(device), y.to(device)

                optimizer.zero_grad()
                out = model(x)
                loss = loss_fn(out, y)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                preds = out.argmax(dim=1)
                correct += (preds == y).sum().item()
                total += y.size(0)
                batch_count += 1

            # Also train on replay buffer if available
            if replay_loader:
                for x, y in replay_loader:
                    x, y = x.to(device), y.to(device)

                    optimizer.zero_grad()
                    out = model(x)
                    loss = loss_fn(out, y)
                    loss.backward()
                    optimizer.step()

                    total_loss += loss.item()
                    preds = out.argmax(dim=1)
                    correct += (preds == y).sum().item()
                    total += y.size(0)
                    batch_count += 1

            avg_loss = total_loss / batch_count if batch_count > 0 else 0
            accuracy = correct / total if total > 0 else 0
            
            report["loss_per_epoch"].append(avg_loss)
            report["accuracy_per_epoch"].append(accuracy)
            
            print(f"Epoch {ep+1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")

        # -------------------------------------------------------
        # Update replay buffer
        # -------------------------------------------------------
        print("Updating replay buffer...")
        new_buffer_items = []

        # Extract samples from new data
        for x, y in train_loader:
            for i in range(len(x)):
                new_buffer_items.append((x[i].cpu(), int(y[i])))

        print(f"New samples: {len(new_buffer_items)}")
        print(f"Old replay: {len(replay_buffer)}")

        # Merge old + new
        combined = replay_buffer + new_buffer_items

        # Keep replay size fixed with random sampling
        if len(combined) > replay_size:
            random.shuffle(combined)
            combined = combined[:replay_size]

        print(f"Final replay buffer: {len(combined)} samples")

        # -------------------------------------------------------
        # Save updated replay buffer
        # -------------------------------------------------------
        os.makedirs(os.path.dirname(args.updated_replay_buffer) or ".", exist_ok=True)
        with open(args.updated_replay_buffer, "wb") as f:
            pickle.dump(combined, f)

        # -------------------------------------------------------
        # Save updated model
        # -------------------------------------------------------
        os.makedirs(os.path.dirname(args.updated_model) or ".", exist_ok=True)
        torch.save(model.state_dict(), args.updated_model)

        # -------------------------------------------------------
        # Save training report
        # -------------------------------------------------------
        os.makedirs(os.path.dirname(args.training_report) or ".", exist_ok=True)
        with open(args.training_report, "w") as f:
            json.dump(report, f, indent=2)

        print("✓ Continual Learning update complete!")
        print(f"✓ Model saved: {args.updated_model}")
        print(f"✓ Replay buffer saved: {args.updated_replay_buffer}")
        print(f"✓ Report saved: {args.training_report}")
        print("="*50)

    args:
      - --processed_data_pickle
      - {inputPath: processed_data_pickle}
      - --previous_model
      - {inputPath: previous_model}
      - --previous_replay_buffer
      - {inputPath: previous_replay_buffer}
      - --config_st
      - {inputValue: config_str}
      - --updated_model
      - {outputPath: updated_model}
      - --updated_replay_buffer
      - {outputPath: updated_replay_buffer}
      - --training_report
      - {outputPath: training_report}

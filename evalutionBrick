name: evaluationBrick
description: Evaluates a Forward-Forward CNN model and outputs performance metrics.

inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: config
    type: String

outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v30
    command:
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, json, os, io
        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
        import nesy_factory.CNNs.ffresnet as ffresnet
        print("=== STARTING FF MODEL EVALUATION ===")

        # ---------------------------------------------------------------------
        # 1. SAFE DATASET UNPICKLER (MATCHING TRAIN BRICK EXACTLY)
        # ---------------------------------------------------------------------
        class LabeledDataset:
            def __init__(self, *args, **kwargs):
                obj = None
                if "dataset" in kwargs:
                    obj = kwargs["dataset"]
                elif len(args) == 1:
                    obj = args[0]
                else:
                    obj = kwargs

                self.dataset = getattr(obj, "dataset", None) or \
                               getattr(obj, "data", None) or \
                               getattr(obj, "samples", None) or \
                               obj

            def __len__(self):
                try:
                    return len(self.dataset)
                except:
                    return 100

            def __getitem__(self, idx):
                try:
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    if isinstance(item, dict):
                        return item.get("image_data"), item.get("label", 0)
                except:
                    pass
                return torch.randn(3,224,224), 0
        
        class CustomJSONDataset(LabeledDataset):
            pass

        class DataWrapper:
            def __init__(self, d=None):
                if d:
                    self.__dict__.update(d)

        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name == "LabeledDataset":
                    return LabeledDataset
                if name == "CustomJSONDataset":
                    return CustomJSONDataset
                if name == "DataWrapper":
                    return DataWrapper
                try:
                    return super().find_class(module, name)
                except:
                    class Dummy:
                      def __init__(self, *a, **kw):
                          pass
                      def __call__(self, *a, **kw):
                          return None

                    return Dummy

        # ---------------------------------------------------------------------
        # 2. PATCHED RESNET (SAME AS TRAINING BRICK)
        # ---------------------------------------------------------------------
        class PatchedResNet(ffresnet.ResNet):
            def __init__(self, config):
                setattr(self, "dropout", config.get("dropout", 0.0))
                super().__init__(config)

        ResNet = PatchedResNet

        # ---------------------------------------------------------------------
        # 3. PARSE ARGS
        # ---------------------------------------------------------------------
        parser = argparse.ArgumentParser()
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--metrics", required=True)
        parser.add_argument("--metrics_json", required=True)
        args = parser.parse_args()

        # ---------------------------------------------------------------------
        # 4. LOAD DATASET (MATCH TRAIN BRICK STRUCTURE)
        # ---------------------------------------------------------------------
        with open(args.data_path, "rb") as f:
            processed = SafeUnpickler(io.BytesIO(f.read())).load()

        test_loader = processed.test_loader
        num_classes = processed.num_classes

        # ---------------------------------------------------------------------
        # 5. LOAD CONFIG FOR RESNET (MATCH TRAIN BRICK)
        # ---------------------------------------------------------------------
        cfg = json.loads(args.config)
        ff_cfg = cfg.get("model", {}).get("forward_forward", {})

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        resnet_cfg = {
            "input_channels": 3,
            "output_dim": num_classes,
            "variant": ff_cfg.get("variant", "resnet50"),
            "use_forward_forward": ff_cfg.get("use_forward_forward", True),
            "use_cafo": ff_cfg.get("use_cafo", False),
            "ff_lr": ff_cfg.get("ff_lr", 0.0001),
            "ff_epochs_per_block": ff_cfg.get("ff_epochs_per_block", 50),
            "ff_threshold": ff_cfg.get("threshold", 2.0),
            "ff_blocks": ff_cfg.get("ff_blocks", 4),
            "dropout": ff_cfg.get("dropout", 0.0),
            "device": str(device),
        }

        print("Loading FF-ResNet model...")
        model = ResNet(resnet_cfg).to(device)

        ckpt = torch.load(args.trained_model, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        model.eval()

        criterion = torch.nn.CrossEntropyLoss()

        # ---------------------------------------------------------------------
        # 6. EVALUATION LOOP
        # ---------------------------------------------------------------------
        all_preds = []
        all_targets = []
        total_loss = 0.0

        with torch.no_grad():
            for X, y in test_loader:
                X, y = X.to(device), y.to(device)
                if X.dtype != torch.float32:
                    X = X.float()

                logits = model(X)
                loss = criterion(logits, y)
                total_loss += loss.item()

                preds = logits.argmax(dim=1)
                all_preds.extend(preds.cpu().tolist())
                all_targets.extend(y.cpu().tolist())

        # ---------------------------------------------------------------------
        # 7. METRICS
        # ---------------------------------------------------------------------
        accuracy = accuracy_score(all_targets, all_preds)
        report = classification_report(all_targets, all_preds, digits=4)
        cm = confusion_matrix(all_targets, all_preds)

        metrics_output = {
            "accuracy": accuracy,
            "confusion_matrix": cm.tolist(),
            "classification_report": report,
            "loss": total_loss / len(test_loader)
        }

        # ---------------------------------------------------------------------
        # 8. SAVE OUTPUTS FOR KFP
        # ---------------------------------------------------------------------
        with open(args.metrics_json, "w") as f:
            json.dump(metrics_output, f, indent=2)

        # KFP Metrics File
        with open(args.metrics, "w") as f:
            f.write(f"accuracy: {accuracy}loss: {total_loss / len(test_loader)}")

        message = "evaluted"
        print(message)


    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}

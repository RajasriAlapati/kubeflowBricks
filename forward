name: forward
description: Trains a CNN model (ResNet) using Backpropagation, CAFO, or Forward-Forward based on configuration.

inputs:
  - name: data_path
    type: Dataset
  - name: config
    type: String

outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v34
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet --upgrade pip setuptools wheel
        python3 -m pip install --quiet "torchvision==0.17.0"
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, time, io, traceback
        from torch.utils.data import TensorDataset, DataLoader
        import nesy_factory.CNNs.ffresnet as ffresnet

        # Define the LabeledDataset and CustomJSONDataset classes for unpickling
        class LabeledDataset:
            def __init__(self, *args, **kwargs):
                obj = None
                if "dataset" in kwargs:
                    obj = kwargs["dataset"]
                elif len(args) == 1:
                    obj = args[0]
                else:
                    obj = kwargs

                self.dataset = getattr(obj, "dataset", None) or \
                               getattr(obj, "data", None) or \
                               getattr(obj, "samples", None) or \
                               obj

            def __len__(self):
                try:
                    return len(self.dataset)
                except:
                    return 100

            def __getitem__(self, idx):
                try:
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    if isinstance(item, dict):
                        return item.get("image_data"), item.get("label", 0)
                except:
                    pass
                return torch.randn(3,224,224), 0
        
        class CustomJSONDataset(LabeledDataset):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)

        class DataWrapper:
            def __init__(self, d=None):
                if d:
                    self.__dict__.update(d)

        # Safe unpickler to load datasets safely
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name == "LabeledDataset":
                    return LabeledDataset
                if name == "CustomJSONDataset":
                    return CustomJSONDataset
                if name == "DataWrapper":
                    return DataWrapper
                return super().find_class(module, name)

        # Dataset loading helper
        def extract_tensors(loader):
            X, y = [], []
            for bx, by in loader:
                X.append(bx)
                y.append(by)
            return torch.cat(X), torch.cat(y)

        # Patched ResNet with backpropagation method
        class PatchedResNet(ffresnet.ResNet):
            def __init__(self, config):
                # Store config first for later use
                self._init_config = config
                
                # Set dropout BEFORE calling super().__init__() 
                # because parent class needs it during initialization
                self.dropout = config.get('dropout', 0.0)
                super().__init__(config)
                
                # Ensure criterion is set (fallback if BaseCNN didn't set it)
                if not hasattr(self, 'criterion') or self.criterion is None:
                    self.criterion = torch.nn.CrossEntropyLoss()
                
                # Store config after parent init to ensure we have it
                self.config = self._init_config
            
            def train_backpropagation(self, X_train, y_train, X_val=None, y_val=None, verbose=True):
                '''Standard backpropagation training method - reads ALL params from config.'''
                # All parameters come from config (no defaults)
                lr = self.config['learning_rate']
                num_epochs = self.config['epochs']
                batch_size = self.config['batch_size']
                
                if verbose:
                    print(f"Backpropagation training with LR={lr}, Epochs={num_epochs}, Batch size={batch_size}")
                
                optimizer = torch.optim.Adam(self.parameters(), lr=lr)
                criterion = self.criterion
                
                # Move data to device
                device = next(self.parameters()).device
                X_train, y_train = X_train.to(device), y_train.to(device)
                if X_val is not None:
                    X_val, y_val = X_val.to(device), y_val.to(device)

                # Create dataloaders
                train_dataset = TensorDataset(X_train, y_train)
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

                # Training loop
                self.train()
                train_losses = []
                val_losses = []
                
                for epoch in range(num_epochs):
                    epoch_loss = 0.0
                    for batch_X, batch_y in train_loader:
                        optimizer.zero_grad()
                        outputs = self(batch_X)
                        loss = criterion(outputs, batch_y)
                        loss.backward()
                        optimizer.step()
                        epoch_loss += loss.item()
                    
                    avg_train_loss = epoch_loss / len(train_loader)
                    train_losses.append(avg_train_loss)

                    # Validation
                    if X_val is not None and y_val is not None:
                        self.eval()
                        with torch.no_grad():
                            val_outputs = self(X_val)
                            val_loss = criterion(val_outputs, y_val)
                            val_losses.append(val_loss.item())
                        self.train()

                    if verbose and epoch % 10 == 0:
                        val_info = f", Val Loss: {val_losses[-1]:.4f}" if val_losses else ""
                        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}{val_info}")

                return {
                    "train_losses": train_losses,
                    "val_losses": val_losses if val_losses else None,
                    "total_epochs": num_epochs
                }

        ResNet = PatchedResNet

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        args = parser.parse_args()

        # Load dataset
        print("Loading dataset...")
        with open(args.data_path, "rb") as f:
            processed = SafeUnpickler(io.BytesIO(f.read())).load()

        # Extract data
        train_loader = processed.train_loader
        test_loader = getattr(processed, "test_loader", None)
        num_classes = processed.num_classes
        image_size = processed.image_size

        X_train, y_train = extract_tensors(train_loader)
        X_val, y_val = (extract_tensors(test_loader) if test_loader else (None, None))

        # Load and parse config
        print("Loading configuration...")
        cfg = json.loads(args.config)
        model_cfg = cfg.get("model", {})
        training_cfg = cfg.get("training", {})
        
        # Extract training mode flags from config
        use_cafo = model_cfg.get("use_cafo")
        use_forward_forward = model_cfg.get("use_forward_forward")
        
        # Validate: can't use both CAFO and FF
        if use_cafo and use_forward_forward:
            raise ValueError("Cannot use both CAFO and Forward-Forward. Set one to false in config.")
        
        # Determine training mode
        if use_cafo:
            training_mode = "CAFO"
        elif use_forward_forward:
            training_mode = "Forward-Forward"
        else:
            training_mode = "Backpropagation"
        
        print(f"Training mode: {training_mode}")

        # Build ResNet config from input config - ALL from input, no defaults
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Extract optimizer config
        optimizer_cfg = training_cfg.get("optimizer", {})
        
        resnet_cfg = {
            # Basic model params - from model section
            "input_channels": model_cfg.get("input_channels"),
            "output_dim": num_classes,
            "variant": model_cfg.get("variant"),
            "pretrained": model_cfg.get("pretrained"),
            "dropout": model_cfg.get("dropout"),
            "device": str(device),
            
            # Training mode flags
            "use_cafo": use_cafo if use_cafo is not None else False,
            "use_forward_forward": use_forward_forward if use_forward_forward is not None else False,
            
            # Training params - from training section (for backpropagation)
            "learning_rate": optimizer_cfg.get("learning_rate"),
            "batch_size": training_cfg.get("batch_size"),
            "epochs": training_cfg.get("epochs", 50),  # You can add this to your config or keep default
        }
        
        # Add CAFO-specific params if CAFO mode
        if use_cafo:
            resnet_cfg.update({
                "cafo_blocks": model_cfg.get("cafo_blocks"),
                "epochs_per_block": model_cfg.get("epochs_per_block"),
                "block_lr": model_cfg.get("block_lr"),
            })
        
        # Add Forward-Forward params if FF mode
        if use_forward_forward:
            ff_cfg = model_cfg.get("forward_forward", {})
            resnet_cfg.update({
                "ff_blocks": ff_cfg.get("ff_blocks"),
                "ff_epochs_per_block": ff_cfg.get("ff_epochs_per_block"),
                "ff_lr": ff_cfg.get("ff_lr"),
                "ff_threshold": ff_cfg.get("threshold"),
                "ff_goodness_dim": ff_cfg.get("ff_goodness_dim"),
            })
        
        # Print config being used
        print(f"\\nConfiguration:")
        print(f"  Variant: {resnet_cfg['variant']}")
        print(f"  Input channels: {resnet_cfg['input_channels']}")
        print(f"  Output dim: {resnet_cfg['output_dim']}")
        print(f"  Dropout: {resnet_cfg['dropout']}")
        if training_mode == "Backpropagation":
            print(f"  Learning rate: {resnet_cfg['learning_rate']}")
            print(f"  Batch size: {resnet_cfg['batch_size']}")
            print(f"  Epochs: {resnet_cfg['epochs']}")
        elif training_mode == "CAFO":
            print(f"  CAFO blocks: {resnet_cfg['cafo_blocks']}")
            print(f"  Epochs per block: {resnet_cfg['epochs_per_block']}")
            print(f"  Block LR: {resnet_cfg['block_lr']}")
        elif training_mode == "Forward-Forward":
            print(f"  FF blocks: {resnet_cfg['ff_blocks']}")
            print(f"  FF epochs per block: {resnet_cfg['ff_epochs_per_block']}")
            print(f"  FF LR: {resnet_cfg['ff_lr']}")
            print(f"  FF threshold: {resnet_cfg['ff_threshold']}")

        # Create model
        print(f"Creating {resnet_cfg['variant']} model...")
        net = ResNet(resnet_cfg).to(device)
        print(f"Model created with {sum(p.numel() for p in net.parameters())} parameters")

        # Train based on mode
        print(f"\\nStarting {training_mode} training...")
        
        if use_cafo:
            results = net.train_cafo(
                X_train.to(device),
                y_train.to(device),
                X_val.to(device) if X_val is not None else None,
                y_val.to(device) if y_val is not None else None,
                verbose=True
            )
            model_trained_flag = {"cafo_trained": True}
            
        elif use_forward_forward:
            results = net.train_forward_forward(
                X_train.to(device),
                y_train.to(device),
                X_val.to(device) if X_val is not None else None,
                y_val.to(device) if y_val is not None else None,
                verbose=True
            )
            model_trained_flag = {"ff_trained": True}
            
        else:
            results = net.train_backpropagation(
                X_train.to(device),
                y_train.to(device),
                X_val.to(device) if X_val is not None else None,
                y_val.to(device) if y_val is not None else None,
                verbose=True
            )
            model_trained_flag = {"backprop_trained": True}

        # Save model & history
        print("\\nSaving model and training history...")
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.training_history), exist_ok=True)

        # Save model state
        save_dict = {
            "model_state_dict": net.state_dict(),
            "config": resnet_cfg,
            "training_mode": training_mode
        }
        save_dict.update(model_trained_flag)
        torch.save(save_dict, args.trained_model)

        # Save training history
        history = {
            "training_mode": training_mode,
            "results": results,
            "config": resnet_cfg
        }
        with open(args.training_history, "w") as f:
            json.dump(history, f, indent=2)

        print(f"\\n✓ {training_mode} training completed successfully!")
        print(f"✓ Model saved to: {args.trained_model}")
        print(f"✓ History saved to: {args.training_history}")
        
    args:
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
